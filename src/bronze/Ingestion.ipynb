{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb8c81ad-3a4e-4681-9663-5846fe2bb1fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Import"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import delta\n",
    "\n",
    "\n",
    "def table_exists(catalog, database, table):\n",
    "  count = (spark.sql(f\"SHOW TABLES FROM {catalog}.{database}\")\n",
    "           .filter(f\"database = '{database}' AND tableName = '{table}' \")\n",
    "           .count())\n",
    "  \n",
    "  return count == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac13c28a-ae0d-4f16-a0f5-bd549ea92150",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "catalog = \"bronze\"\n",
    "schema = \"f1_world\"\n",
    "tableName = dbutils.widgets.get(\"tableName\")\n",
    "output_streaming = f\"{source_path}streaming_source/\"\n",
    "source_path = f\"/Volumes/raw/{schema}/\"\n",
    "TARGET_RACEID = 1141\n",
    "files_split = [\"constructor_results\" ,\"constructor_standings\", \"driver standings\", \"lap_times\", \"pit_stops\", \"qualifying\", \"results\", \"sprint_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2e4f934-adce-49cd-aec2-73ef061dae4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Ingestor:\n",
    "\n",
    "    def __init__(self, catalog, schemaname, tablename, data_format, target_race_id):\n",
    "        self.catalog = catalog\n",
    "        self.schemaname = schemaname\n",
    "        self.tablename = tablename\n",
    "        self.data_format = data_format\n",
    "        self.target_race_id = target_race_id\n",
    "        self.set_schema()\n",
    "\n",
    "\n",
    "\n",
    "    def set_schema(self):\n",
    "        self.data_schema = dbutils.import_schema(self.tablename)\n",
    "\n",
    "\n",
    "    def load(self,path):\n",
    "        df = (spark.read\n",
    "            .format(self.data_format)\n",
    "            .schema(self.data_schema)\n",
    "            .options(header=\"true\")\n",
    "            .option(\"nullValue\", \"\\\\N\")\n",
    "            .load(path))\n",
    "        return df\n",
    "        \n",
    "    def save_batch(self,df):\n",
    "        (df.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{self.catalog}.{self.schemanema}.{self.tablename}\"))\n",
    "        \n",
    "\n",
    "    def save_streaming(self,df):\n",
    "        (df.coalesce(1)\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .save(f\"/Volumes/raw/{self.schemaname}/streaming_source.{self.tablename}\"))\n",
    "\n",
    "\n",
    "    def execute_batch(self,path):\n",
    "        df = self.load(path)\n",
    "\n",
    "        if self.target_race_id:\n",
    "            print(f\"Filtrando Batch sem a corrida {self.target_race_id}...\")\n",
    "            df_batch = df.filter(df.raceId != self.target_race_id)\n",
    "            self.save_batch(df_batch)\n",
    "        else:\n",
    "            self.save_batch(df)\n",
    "\n",
    "    def execute_streaming(self,path):\n",
    "        df = self.load(path)\n",
    "        \n",
    "        if self.target_race_id:\n",
    "            print(f\"Filtrando Streaming da corrida {self.target_race_id}...\")\n",
    "            df_streaming = df.filter(df.raceId == self.target_race_id)\n",
    "            self.save_streaming(df_streaming)\n",
    "        else:\n",
    "            self.save_streaming(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4efca4e4-2414-44e8-9374-1bf31ebbedea",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Ingest√£o Full Load e Split"
    }
   },
   "outputs": [],
   "source": [
    "if not table_exists(catalog, schema, tableName):\n",
    "  print(f\"Criando tabela {tableName}\")\n",
    "\n",
    "  if tableName in files_split:\n",
    "\n",
    "    df_split = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .options(header=\"true\")\n",
    "          .load(f\"{source_path}{tableName}/\"))\n",
    "    \n",
    "    # Consolidando full load sem a corrida alvo\n",
    "    df_batch = df_split.filter(df_split.raceId != TARGET_RACEID)\n",
    "    (df_batch.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{tableName}\"))\n",
    "\n",
    "    print(\"Tabela delta criado com sucesso\")\n",
    "    \n",
    "    # Separando streaming\n",
    "    df_streaming = df_split.filter(df_split.raceId == TARGET_RACEID)\n",
    "    (df_streaming.coalesce(1)\n",
    "            .write\n",
    "            .format(\"parquet\")\n",
    "            .mode(\"overwrite\")\n",
    "            .save(f\"{output_streaming}{tableName}\"))\n",
    "\n",
    "  else:\n",
    "    df_full = (spark.read\n",
    "          .format(\"csv\")\n",
    "          .options(header=\"true\")\n",
    "          .load(f\"{source_path}{tableName}/\"))\n",
    "\n",
    "    (df_full.coalesce(1)\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .saveAsTable(f\"{catalog}.{schema}.{tableName}\"))\n",
    "else:\n",
    "  print(f\"Tabela {tableName} ja existe\")\n",
    "                                        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8413021898016102,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
